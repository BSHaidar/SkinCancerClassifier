{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !unzip '../SkinCancerClassifier/Archive.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# image_test_list=[]\n",
    "# for i in range(0,7):\n",
    "#     sub_dir= image_test_dir + '/' + str(i)\n",
    "#     for sub in os.listdir(sub_dir):\n",
    "#         image_test_list.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers.convolutional import *\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import BatchNormalization, Dropout, AveragePooling2D, GlobalAvgPool2D, MaxPooling2D\n",
    "from keras.applications import inception_v3\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.random import seed\n",
    "seed(111)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test_dir = '../SkinCancerClassifier/test_dir/'\n",
    "image_train_dir = '../SkinCancerClassifier/train_dir/'\n",
    "image_val_dir = '../SkinCancerClassifier/valid_dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_name(dir_name):\n",
    "    img_list = []\n",
    "    for i in range(0, 7):\n",
    "        sub_dir= dir_name + str(i) +'/'\n",
    "        for sub in os.listdir(sub_dir):\n",
    "            img_list.append(sub)\n",
    "    return img_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_dict():\n",
    "\n",
    "    \n",
    "    data_dir = '../SkinCancerClassifier/'\n",
    "    # Create dataframe and profile raw data\n",
    "    raw_metadata_df = pd.read_csv(data_dir + 'HAM10000_metadata.csv')\n",
    "    image_test_dir = '../SkinCancerClassifier/test_dir/'\n",
    "    image_train_dir = '../SkinCancerClassifier/train_dir/'\n",
    "    image_val_dir = '../SkinCancerClassifier/valid_dir/'\n",
    "    img_test_list = img_name(image_test_dir)\n",
    "    img_test_list = [image_test_dir + img_name for img_name in img_test_list]\n",
    "    img_train_list = img_name(image_train_dir)\n",
    "    img_train_list = [image_train_dir + img_name for img_name in img_train_list]\n",
    "    img_val_list = img_name(image_val_dir)\n",
    "    img_val_list = [image_val_dir + img_name for img_name in img_val_list]\n",
    "    all_img_list = img_test_list + img_train_list + img_val_list\n",
    "    image_dict = {os.path.splitext(os.path.basename(img_name))[0]: img_name for img_name in all_img_list}\n",
    "    \n",
    "    return raw_metadata_df, image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_df():\n",
    "    # Create dictionary with the diagnostic categories of pigmented lesions\n",
    "    lesion_cat_dict = {\n",
    "        \n",
    "        'akiec': 'Actinic keratoses',\n",
    "        'bcc': 'Basal cell carcinoma',\n",
    "        'bkl': 'Benign keratosis-like lesions ',\n",
    "        'df': 'Dermatofibroma',\n",
    "        'mel': 'Melanoma',\n",
    "        'nv': 'Melanocytic nevi',\n",
    "        'vasc': 'Vascular lesions'\n",
    "    }\n",
    "\n",
    "    # Make a copy of the dataframe as we will be adding new columns\n",
    "    df, img_dict = create_img_dict()\n",
    "\n",
    "\n",
    "    # Create new column file_path and use the image_id as the key of image_dict and map \n",
    "    # its corresponding value to get the path for the image\n",
    "    df['file_path'] = df['image_id'].map(img_dict.get)\n",
    "\n",
    "    # Create new column category_name and use dx as the key to lesion_cat_dict and map \n",
    "    # it to its corresponding value to get the lesion name\n",
    "    df['category_name'] = df['dx'].map(lesion_cat_dict.get)\n",
    "\n",
    "    # Create new column category_id and assign the integer codes \n",
    "    # of the category_name that was transformed into a pandas categorical \n",
    "    df['category_id'] = pd.Categorical(df['category_name']).codes\n",
    "\n",
    "    # fill age null values by the mean age\n",
    "    df.age.fillna(df.age.mean(), inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_images(train_dir=image_train_dir, test_dir=image_test_dir, val_dir=image_val_dir):\n",
    "  \n",
    "    train_destination = train_dir\n",
    "    test_destination = test_dir\n",
    "    valid_destination = val_dir\n",
    "\n",
    "    # get all the data in the directory split/test, and reshape them\n",
    "    test_data = ImageDataGenerator(featurewise_center=True,\n",
    "                                    featurewise_std_normalization=True,\n",
    "                                    rotation_range=20,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    horizontal_flip=True,rescale=1./255).flow_from_directory( \n",
    "                                    test_destination, \n",
    "                                    target_size=(112, 150),\n",
    "                                    batch_size = 2000, \n",
    "                                    seed = 1212) \n",
    "\n",
    "    train_data = ImageDataGenerator(rescale=1./255).flow_from_directory( \n",
    "            train_destination, \n",
    "            target_size=(112, 150), \n",
    "            batch_size = 49, \n",
    "            seed = 1212) \n",
    "\n",
    "    valid_data = ImageDataGenerator(rescale=1./255).flow_from_directory( \n",
    "            valid_destination, \n",
    "            target_size=(112, 150),\n",
    "            batch_size = 49,\n",
    "            seed = 1212) \n",
    "\n",
    "#     #split images and labels for train, test, and validation\n",
    "#     images_train, labels_train = next(train_data)\n",
    "\n",
    "#     images_test, labels_test = next(test_data)\n",
    "\n",
    "#     images_val, labels_val = next(valid_data)\n",
    "    \n",
    "#     return images_train, labels_train, images_test, labels_test, images_val, labels_val\n",
    "    return test_data, train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cnn_model():\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', dilation_rate=(2, 2), kernel_regularizer=l2(0.01), input_shape=(112, 150,  3)))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    # cnn.add(Dropout(0.50)) # Added to see if it helps in reducing overfitting\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(32, activation='relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    print(cnn.summary())\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cnn_model(cnn, loss_param='categorical_crossentropy', epoch=20, batch_num=32):\n",
    "    \n",
    "#     images_train, labels_train, images_test, labels_test, images_val, labels_val = split_images()\n",
    "    test_data, train_data, valid_data = split_images()\n",
    "    optimize = SGD(lr=1e-2, momentum=0.9) \n",
    "    \n",
    "    cnn.compile(loss=loss_param,\n",
    "                    optimizer=optimize,\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "\n",
    "#     cnn_fit = cnn.fit(images_train,\n",
    "#                         labels_train,\n",
    "#                         epochs=epoch,\n",
    "#                         batch_size=batch_num,\n",
    "#                         validation_data=(images_val, labels_val))\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='../SkinCancerClassifier/saved_models/CNN_2.h5', verbose=1, save_best_only=True)\n",
    "    cnn.fit_generator(train_data,\n",
    "                       steps_per_epoch=np.ceil(train_data.samples/train_data.batch_size),\n",
    "                       epochs=epoch, \n",
    "                       verbose=1,\n",
    "                       callbacks=[checkpointer], \n",
    "                       validation_data=valid_data, \n",
    "                       validation_steps=np.ceil(valid_data.samples/valid_data.batch_size), \n",
    "    #                        class_weight=None, \n",
    "    #                        max_queue_size=10, \n",
    "                       workers=16, \n",
    "                       use_multiprocessing=True, \n",
    "                       shuffle=True, \n",
    "                       initial_epoch=0)\n",
    "\n",
    "                      \n",
    "     \n",
    "#     cnn.save('../SkinCancerClassifier/saved_models/CNN_1.h5')\n",
    "    \n",
    "#     return cnn, images_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "# learning rate schedule\n",
    "def step_decay(epoch=5):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = set_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    6705\n",
       "5    1113\n",
       "2    1099\n",
       "1     514\n",
       "0     327\n",
       "6     142\n",
       "3     115\n",
       "Name: category_id, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_id.value_counts() #Serious Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy fewer class to balance the number of 7 classes\n",
    "data_aug_rate = [15,10,5,50,0,40,5]\n",
    "for i in range(7):\n",
    "    if data_aug_rate[i]:\n",
    "        df=df_train.append([df_train.loc[df_train['category_id'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n",
    "df_train['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 108, 146, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 108, 146, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 54, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 252288)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                8073248   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 8,075,655\n",
      "Trainable params: 8,075,463\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = set_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 23s 178ms/step - loss: 1.8865 - acc: 0.6722 - val_loss: 2.4327 - val_acc: 0.6215\n",
      "Epoch 2/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.7809 - acc: 0.6147 - val_loss: 3.0108 - val_acc: 0.2078\n",
      "Epoch 3/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 7.5005 - acc: 0.6115 - val_loss: 20.8160 - val_acc: 0.1262\n",
      "Epoch 4/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 4.9816 - acc: 0.5630 - val_loss: 14.1152 - val_acc: 0.1088\n",
      "Epoch 5/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 2.2639 - acc: 0.5937 - val_loss: 6.5210 - val_acc: 0.1070\n",
      "Epoch 6/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.6806 - acc: 0.5968 - val_loss: 8.8434 - val_acc: 0.0705\n",
      "Epoch 7/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.6529 - acc: 0.6742 - val_loss: 12.5910 - val_acc: 0.1354\n",
      "Epoch 8/20\n",
      "131/131 [==============================] - 20s 155ms/step - loss: 1.4305 - acc: 0.6400 - val_loss: 2.4494 - val_acc: 0.6543\n",
      "Epoch 9/20\n",
      "131/131 [==============================] - 20s 155ms/step - loss: 1.5359 - acc: 0.6004 - val_loss: 5.6986 - val_acc: 0.0878\n",
      "Epoch 10/20\n",
      "131/131 [==============================] - 20s 156ms/step - loss: 1.4071 - acc: 0.5833 - val_loss: 6.8828 - val_acc: 0.1138\n",
      "Epoch 11/20\n",
      "131/131 [==============================] - 24s 180ms/step - loss: 1.3592 - acc: 0.6316 - val_loss: 2.1488 - val_acc: 0.4688\n",
      "Epoch 12/20\n",
      "131/131 [==============================] - 21s 157ms/step - loss: 1.3372 - acc: 0.6305 - val_loss: 1.3100 - val_acc: 0.5566\n",
      "Epoch 13/20\n",
      "131/131 [==============================] - 20s 149ms/step - loss: 1.2440 - acc: 0.6422 - val_loss: 2.6605 - val_acc: 0.6889\n",
      "Epoch 14/20\n",
      "131/131 [==============================] - 19s 145ms/step - loss: 1.5143 - acc: 0.6091 - val_loss: 6.9567 - val_acc: 0.1169\n",
      "Epoch 15/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.5047 - acc: 0.6222 - val_loss: 2.1242 - val_acc: 0.5170\n",
      "Epoch 16/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.3244 - acc: 0.6744 - val_loss: 14.8405 - val_acc: 0.0874\n",
      "Epoch 17/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.6259 - acc: 0.5356 - val_loss: 6.5666 - val_acc: 0.0487\n",
      "Epoch 18/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.6596 - acc: 0.5921 - val_loss: 1.8538 - val_acc: 0.4657\n",
      "Epoch 19/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.5175 - acc: 0.6361 - val_loss: 1.4484 - val_acc: 0.6846\n",
      "Epoch 20/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.3021 - acc: 0.6313 - val_loss: 14.4444 - val_acc: 0.1119\n"
     ]
    }
   ],
   "source": [
    "# model, images_test, labels_test = fit_cnn_model(cnn)\n",
    "fit_cnn_model(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "131/131 [==============================] - 19s 145ms/step - loss: 1.9177 - acc: 0.6934 - val_loss: 6.0087 - val_acc: 0.6419\n",
      "Epoch 2/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 1.6881 - acc: 0.6364 - val_loss: 1.9711 - val_acc: 0.6110\n",
      "Epoch 3/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.3498 - acc: 0.6605 - val_loss: 5.8993 - val_acc: 0.6524\n",
      "Epoch 4/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 1.7048 - acc: 0.5585 - val_loss: 6.3525 - val_acc: 0.4917\n",
      "Epoch 5/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.6291 - acc: 0.6297 - val_loss: 8.8979 - val_acc: 0.0396\n",
      "Epoch 6/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.6717 - acc: 0.6171 - val_loss: 3.2268 - val_acc: 0.4805\n",
      "Epoch 7/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.8330 - acc: 0.6509 - val_loss: 6.3996 - val_acc: 0.5931\n",
      "Epoch 8/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 2.8264 - acc: 0.5904 - val_loss: 4.7455 - val_acc: 0.2059\n",
      "Epoch 9/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 2.0813 - acc: 0.5630 - val_loss: 14.6777 - val_acc: 0.1138\n",
      "Epoch 10/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 1.9252 - acc: 0.5895 - val_loss: 5.8475 - val_acc: 0.6160\n",
      "Epoch 11/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.6674 - acc: 0.6191 - val_loss: 3.5173 - val_acc: 0.0946\n",
      "Epoch 12/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4628 - acc: 0.6333 - val_loss: 9.4614 - val_acc: 0.4174\n",
      "Epoch 13/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5504 - acc: 0.6189 - val_loss: 1.7316 - val_acc: 0.6351\n",
      "Epoch 14/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5700 - acc: 0.5940 - val_loss: 6.9898 - val_acc: 0.0754\n",
      "Epoch 15/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 1.5920 - acc: 0.6284 - val_loss: 16.0124 - val_acc: 0.0204\n",
      "Epoch 16/20\n",
      "131/131 [==============================] - 14s 110ms/step - loss: 1.1758 - acc: 0.6796 - val_loss: 15.4076 - val_acc: 0.0516\n",
      "Epoch 17/20\n",
      "131/131 [==============================] - 15s 111ms/step - loss: 1.6355 - acc: 0.5721 - val_loss: 8.6051 - val_acc: 0.0268\n",
      "Epoch 18/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5935 - acc: 0.5960 - val_loss: 1.4894 - val_acc: 0.6704\n",
      "Epoch 19/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.2454 - acc: 0.6666 - val_loss: 6.7084 - val_acc: 0.5071\n",
      "Epoch 20/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.3715 - acc: 0.6314 - val_loss: 2.3759 - val_acc: 0.5213\n"
     ]
    }
   ],
   "source": [
    "fit_cnn_model(cnn, batch_num=4, epoch=20) # momentum 0.9 instead of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 73s 556ms/step - loss: 1.9724 - acc: 0.6537 - val_loss: 5.3504 - val_acc: 0.6079\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 1.6108 - acc: 0.6457 - val_loss: 4.5567 - val_acc: 0.6704\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 1.3003 - acc: 0.6580 - val_loss: 2.5958 - val_acc: 0.6438\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 1.5212 - acc: 0.5840 - val_loss: 1.6760 - val_acc: 0.6419\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 1.5086 - acc: 0.6337 - val_loss: 3.9206 - val_acc: 0.4422\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 1.3694 - acc: 0.6258 - val_loss: 14.0305 - val_acc: 0.0792\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 1.6446 - acc: 0.6817 - val_loss: 12.8127 - val_acc: 0.1194\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 67s 515ms/step - loss: 1.4641 - acc: 0.6158 - val_loss: 4.6393 - val_acc: 0.5615\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 68s 516ms/step - loss: 1.7237 - acc: 0.5529 - val_loss: 3.2440 - val_acc: 0.4230\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 68s 517ms/step - loss: 1.5622 - acc: 0.6216 - val_loss: 9.9588 - val_acc: 0.3080\n"
     ]
    }
   ],
   "source": [
    "fit_cnn_model(cnn, batch_num=10, epoch=10) #(225 x 300 px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "131/131 [==============================] - 19s 146ms/step - loss: 1.8186 - acc: 0.6739 - val_loss: 2.8630 - val_acc: 0.5807\n",
      "Epoch 2/20\n",
      "131/131 [==============================] - 15s 113ms/step - loss: 1.6771 - acc: 0.6337 - val_loss: 10.2280 - val_acc: 0.0952\n",
      "Epoch 3/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4749 - acc: 0.6481 - val_loss: 4.1536 - val_acc: 0.6524\n",
      "Epoch 4/20\n",
      "131/131 [==============================] - 15s 113ms/step - loss: 1.6929 - acc: 0.5948 - val_loss: 4.7951 - val_acc: 0.6129\n",
      "Epoch 5/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.7379 - acc: 0.6068 - val_loss: 5.5606 - val_acc: 0.6648\n",
      "Epoch 6/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4936 - acc: 0.6051 - val_loss: 5.8686 - val_acc: 0.6289\n",
      "Epoch 7/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.2751 - acc: 0.6841 - val_loss: 7.3019 - val_acc: 0.4088\n",
      "Epoch 8/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.3203 - acc: 0.6274 - val_loss: 5.8917 - val_acc: 0.0761\n",
      "Epoch 9/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4585 - acc: 0.5702 - val_loss: 6.6631 - val_acc: 0.3908\n",
      "Epoch 10/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4961 - acc: 0.5822 - val_loss: 15.2036 - val_acc: 0.0680\n",
      "Epoch 11/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.8006 - acc: 0.5939 - val_loss: 6.7547 - val_acc: 0.5993\n",
      "Epoch 12/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5689 - acc: 0.6300 - val_loss: 4.6944 - val_acc: 0.7137\n",
      "Epoch 13/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5695 - acc: 0.6166 - val_loss: 14.7100 - val_acc: 0.0099\n",
      "Epoch 14/20\n",
      "131/131 [==============================] - 15s 113ms/step - loss: 1.7757 - acc: 0.5923 - val_loss: 4.5859 - val_acc: 0.3797\n",
      "Epoch 15/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.6557 - acc: 0.6289 - val_loss: 5.1103 - val_acc: 0.6957\n",
      "Epoch 16/20\n",
      "131/131 [==============================] - 15s 113ms/step - loss: 1.5249 - acc: 0.6665 - val_loss: 14.0520 - val_acc: 0.1032\n",
      "Epoch 17/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.9461 - acc: 0.5573 - val_loss: 4.9513 - val_acc: 0.7024\n",
      "Epoch 18/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.5442 - acc: 0.6094 - val_loss: 14.4768 - val_acc: 0.1113\n",
      "Epoch 19/20\n",
      "131/131 [==============================] - 15s 112ms/step - loss: 1.4356 - acc: 0.6593 - val_loss: 3.1878 - val_acc: 0.5572\n",
      "Epoch 20/20\n",
      "131/131 [==============================] - 15s 113ms/step - loss: 3.6705 - acc: 0.6197 - val_loss: 20.1119 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "fit_cnn_model(cnn, batch_num=32, epoch=20) #(112 x 150 px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cnn_bn_l2_model():\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(Conv2D(128, (1, 1), activation='relu', kernel_initializer='he_uniform', dilation_rate=(2, 2), kernel_regularizer=l2(0.01),input_shape=(112, 150, 3)))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.02)))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(MaxPooling2D((2, 2)))\n",
    "#     cnn.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(1)))\n",
    "#     cnn.add(layers.BatchNormalization())\n",
    "#     cnn.add(layers.MaxPooling2D((2, 2))) \n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(layers.Dense(32, activation='relu', kernel_regularizer=l2(0.02)))\n",
    "    cnn.add(Dropout(0.5))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    print(cnn.summary())\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cnn_bn_l2_model(cnn, batch_num=32, loss_param='categorical_crossentropy', epoch=40):\n",
    "  \n",
    "    callbacks_list = []\n",
    "    test_data, train_data, valid_data = split_images()\n",
    "   \n",
    "    rlrop = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=5) \n",
    "    \n",
    "    \n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay = 0.1, nesterov=False)\n",
    "    \n",
    "    cnn.compile(loss=loss_param,\n",
    "                optimizer=sgd,\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [lrate]\n",
    "#     checkpointer = ModelCheckpoint(filepath='../SkinCancerClassifier/saved_models/CNN_bnn_l2.h5', verbose=1, save_best_only=True)\n",
    "#     callbacks_list.append(checkpointer)\n",
    "    cnn.fit_generator(train_data,\n",
    "                       steps_per_epoch=np.ceil(train_data.samples/train_data.batch_size),\n",
    "                       epochs=epoch, \n",
    "                       verbose=1,\n",
    "                       callbacks=callbacks_list, \n",
    "                       validation_data=valid_data, \n",
    "                       validation_steps=np.ceil(valid_data.samples/valid_data.batch_size), \n",
    "                       workers=16, \n",
    "                       use_multiprocessing=True, \n",
    "                       shuffle=True, \n",
    "                       initial_epoch=0)\n",
    "\n",
    "#     cnn_fit = cnn.fit(images_train,\n",
    "#                         labels_train,\n",
    "#                         epochs=epoch,\n",
    "#                         batch_size=batch_num,\n",
    "#                         validation_data=(images_val, labels_val),\n",
    "#                         callbacks=[rlrop])\n",
    "    \n",
    "    cnn.save('/Users/basselhaidar/Desktop/Final Project/saved_models/CNN_bnn_1.h5')\n",
    "    \n",
    "#     return cnn, images_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 112, 150, 128)     512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 112, 150, 128)     512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 56, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 54, 73, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 54, 73, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 27, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 62208)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1990688   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 2,066,119\n",
      "Trainable params: 2,065,671\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_bnn = set_cnn_bn_l2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "131/131 [==============================] - 38s 290ms/step - loss: 13.7900 - acc: 0.7091 - val_loss: 8.1201 - val_acc: 0.6518\n",
      "Epoch 2/40\n",
      "131/131 [==============================] - 33s 248ms/step - loss: 6.1492 - acc: 0.7126 - val_loss: 4.9704 - val_acc: 0.6704\n",
      "Epoch 3/40\n",
      "131/131 [==============================] - 33s 249ms/step - loss: 4.3401 - acc: 0.7116 - val_loss: 3.9432 - val_acc: 0.6425\n",
      "Epoch 4/40\n",
      "131/131 [==============================] - 33s 249ms/step - loss: 3.7180 - acc: 0.6590 - val_loss: 3.3380 - val_acc: 0.7044\n",
      "Epoch 5/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 3.1705 - acc: 0.6884 - val_loss: 3.0065 - val_acc: 0.6957\n",
      "Epoch 6/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 2.8307 - acc: 0.6830 - val_loss: 2.7867 - val_acc: 0.6951\n",
      "Epoch 7/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.5653 - acc: 0.7260 - val_loss: 2.5078 - val_acc: 0.6574\n",
      "Epoch 8/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 2.4058 - acc: 0.7079 - val_loss: 2.3130 - val_acc: 0.6747\n",
      "Epoch 9/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.3757 - acc: 0.6649 - val_loss: 2.2139 - val_acc: 0.7069\n",
      "Epoch 10/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 2.2138 - acc: 0.6811 - val_loss: 2.2591 - val_acc: 0.6784\n",
      "Epoch 11/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.1304 - acc: 0.6936 - val_loss: 2.1705 - val_acc: 0.6252\n",
      "Epoch 12/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.1019 - acc: 0.7023 - val_loss: 1.9975 - val_acc: 0.7644\n",
      "Epoch 13/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.0970 - acc: 0.6900 - val_loss: 1.9791 - val_acc: 0.7359\n",
      "Epoch 14/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.1335 - acc: 0.6509 - val_loss: 1.9206 - val_acc: 0.7353\n",
      "Epoch 15/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.0807 - acc: 0.6836 - val_loss: 1.9386 - val_acc: 0.7285\n",
      "Epoch 16/40\n",
      "131/131 [==============================] - 33s 253ms/step - loss: 1.9592 - acc: 0.7131 - val_loss: 1.7804 - val_acc: 0.7856\n",
      "Epoch 17/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 2.0907 - acc: 0.6618 - val_loss: 1.8468 - val_acc: 0.7344\n",
      "Epoch 18/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.9764 - acc: 0.6869 - val_loss: 1.8023 - val_acc: 0.7242\n",
      "Epoch 19/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.8923 - acc: 0.6978 - val_loss: 1.8400 - val_acc: 0.7180\n",
      "Epoch 20/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8876 - acc: 0.6909 - val_loss: 1.7732 - val_acc: 0.7335\n",
      "Epoch 21/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8479 - acc: 0.7006 - val_loss: 1.7500 - val_acc: 0.7019\n",
      "Epoch 22/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.9082 - acc: 0.6841 - val_loss: 1.8569 - val_acc: 0.6926\n",
      "Epoch 23/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.9088 - acc: 0.6920 - val_loss: 1.7106 - val_acc: 0.7403\n",
      "Epoch 24/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.8095 - acc: 0.7183 - val_loss: 1.8131 - val_acc: 0.7526\n",
      "Epoch 25/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7867 - acc: 0.7215 - val_loss: 1.9151 - val_acc: 0.6982\n",
      "Epoch 26/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8505 - acc: 0.7028 - val_loss: 1.7577 - val_acc: 0.7001\n",
      "Epoch 27/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8105 - acc: 0.7292 - val_loss: 1.8047 - val_acc: 0.6914\n",
      "Epoch 28/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7804 - acc: 0.7084 - val_loss: 1.6939 - val_acc: 0.7279\n",
      "Epoch 29/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8115 - acc: 0.7182 - val_loss: 1.7186 - val_acc: 0.7285\n",
      "Epoch 30/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7923 - acc: 0.6996 - val_loss: 1.7436 - val_acc: 0.7199\n",
      "Epoch 31/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.8238 - acc: 0.7003 - val_loss: 1.8241 - val_acc: 0.6945\n",
      "Epoch 32/40\n",
      "131/131 [==============================] - 32s 245ms/step - loss: 1.8005 - acc: 0.7046 - val_loss: 1.8737 - val_acc: 0.6417\n",
      "Epoch 33/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.7761 - acc: 0.6900 - val_loss: 1.6833 - val_acc: 0.7375\n",
      "Epoch 34/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7666 - acc: 0.7034 - val_loss: 1.7627 - val_acc: 0.6685\n",
      "Epoch 35/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.7977 - acc: 0.6929 - val_loss: 1.5966 - val_acc: 0.7434\n",
      "Epoch 36/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7713 - acc: 0.7112 - val_loss: 1.7381 - val_acc: 0.7427\n",
      "Epoch 37/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.7635 - acc: 0.7336 - val_loss: 1.8481 - val_acc: 0.7112\n",
      "Epoch 38/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.6692 - acc: 0.7560 - val_loss: 1.8083 - val_acc: 0.6691\n",
      "Epoch 39/40\n",
      "131/131 [==============================] - 33s 251ms/step - loss: 1.7704 - acc: 0.7218 - val_loss: 1.7071 - val_acc: 0.6994\n",
      "Epoch 40/40\n",
      "131/131 [==============================] - 33s 250ms/step - loss: 1.8201 - acc: 0.6934 - val_loss: 1.6677 - val_acc: 0.7297\n"
     ]
    }
   ],
   "source": [
    "fit_cnn_bn_l2_model(cnn_bnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data, train_data, valid_data = split_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:707: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "te_img, te_labels = next(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 4s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8326410102844237, 0.6865]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_bnn.evaluate(te_img, te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn_bnn.predict_classes(te_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 1, 4, 4, 4, 4, 4, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_labels.argmax(axis=1)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   17,   14,    0,   33,    0,    0],\n",
       "       [   1,   44,   15,    0,   42,    0,    0],\n",
       "       [   3,   11,   62,    0,  142,    1,    0],\n",
       "       [   1,    7,    6,    0,    9,    0,    0],\n",
       "       [   1,   27,   47,    0, 1256,   10,    0],\n",
       "       [   0,    1,   37,    0,  174,   10,    0],\n",
       "       [   0,    8,    3,    0,   17,    0,    0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(te_labels.argmax(axis=1), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate Confusion Matrix\n",
    "cm = confusion_matrix(te_labels.argmax(axis=1), y_pred)\n",
    "\n",
    "# Figure adjustment and heatmap plot\n",
    "f = plt.figure(figsize=(10,10))\n",
    "ax= plt.subplot()\n",
    "labels = df.groupby('category_id').category_name.first().values\n",
    "sns.heatmap(cm, annot=True, ax = ax, vmax=100, cbar=False, cmap='Paired', mask=(cm==0), fmt=',.0f', linewidths=2, linecolor='grey', ); \n",
    "\n",
    "# labels\n",
    "ax.set_xlabel('Predicted labels', fontsize=16);\n",
    "ax.set_ylabel('True labels', labelpad=30, fontsize=16); \n",
    "ax.set_title('Confusion Matrix', fontsize=18); \n",
    "ax.xaxis.set_ticklabels(labels, rotation=90); \n",
    "ax.yaxis.set_ticklabels(labels, rotation=0);\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Actinic keratoses', 'Basal cell carcinoma',\n",
       "       'Benign keratosis-like lesions ', 'Dermatofibroma',\n",
       "       'Melanocytic nevi', 'Melanoma', 'Vascular lesions'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category_id').category_name.first().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.application.densenet import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import sys\n",
    "sys.path.insert(0, '../SkinCancerClassifier/py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% || 317kB 33.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% || 51kB 27.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% || 51kB 28.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K    100% || 109.2MB 441kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "\u001b[K    100% || 61kB 18.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% || 102kB 38.1MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K    100% || 491kB 33.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% || 3.2MB 15.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5d/b434403adb2db8853a97828d3d19f2032e79d630e0d11a8e95d243103a11/grpcio-1.22.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% || 2.2MB 19.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% || 92kB 38.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Building wheels for collected packages: termcolor, gast, absl-py, wrapt\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "Successfully built termcolor gast absl-py wrapt\n",
      "\u001b[31mtensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: termcolor, google-pasta, gast, absl-py, tensorflow-estimator, markdown, grpcio, tensorboard, wrapt, astor, tensorflow\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mCannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% || 1.4MB 21.6MB/s ta 0:00:01\n",
      "\u001b[31mtensorboard 1.14.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/85/0a5a2ec6ac6214042688ba0dd996ad0ba95fa88b78d578733d9db0b6460d/sagemaker_tensorflow-1.14.0.1.0.0-cp36-cp36m-manylinux1_x86_64.whl (776kB)\n",
      "\u001b[K     || 778kB 35.8MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sagemaker-tensorflow\n",
      "Successfully installed sagemaker-tensorflow-1.14.0.1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from setup_images import *\n",
    "from cnn_classifier import *\n",
    "# import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images = 10015\n",
      "Check path of ISIC_0025030.jpg = /Users/basselhaidar/Desktop/Final Project/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0025030.jpg\n"
     ]
    }
   ],
   "source": [
    "df = set_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 108, 196, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 108, 196, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 338688)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                10838048  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 10,840,455\n",
      "Trainable params: 10,840,263\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = set_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 179s 45ms/step - loss: 2.8599 - acc: 0.5460 - val_loss: 2.5690 - val_acc: 0.5775\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 181s 45ms/step - loss: 2.2538 - acc: 0.6803 - val_loss: 2.0859 - val_acc: 0.6813\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 182s 46ms/step - loss: 1.9838 - acc: 0.7240 - val_loss: 2.0498 - val_acc: 0.6687\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 190s 48ms/step - loss: 1.8256 - acc: 0.7515 - val_loss: 1.9081 - val_acc: 0.6950\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 182s 45ms/step - loss: 1.7029 - acc: 0.7738 - val_loss: 1.8382 - val_acc: 0.7125\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 1.6013 - acc: 0.7885 - val_loss: 1.7518 - val_acc: 0.7137\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 185s 46ms/step - loss: 1.4980 - acc: 0.8095 - val_loss: 1.9197 - val_acc: 0.6663\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 211s 53ms/step - loss: 1.3848 - acc: 0.8315 - val_loss: 1.6835 - val_acc: 0.7000\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 180s 45ms/step - loss: 1.2836 - acc: 0.8610 - val_loss: 1.7778 - val_acc: 0.6925\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 1.1967 - acc: 0.8740 - val_loss: 1.7080 - val_acc: 0.6737\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 1.0938 - acc: 0.8965 - val_loss: 1.6415 - val_acc: 0.6925\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 1.0149 - acc: 0.9155 - val_loss: 1.5959 - val_acc: 0.7075\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 177s 44ms/step - loss: 0.9249 - acc: 0.9380 - val_loss: 1.7154 - val_acc: 0.6963\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 177s 44ms/step - loss: 0.8448 - acc: 0.9500 - val_loss: 1.5314 - val_acc: 0.6925\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.7975 - acc: 0.9600 - val_loss: 1.5927 - val_acc: 0.7013\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.7446 - acc: 0.9655 - val_loss: 1.5044 - val_acc: 0.7063\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.6842 - acc: 0.9748 - val_loss: 1.6098 - val_acc: 0.6813\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 174s 44ms/step - loss: 0.6286 - acc: 0.9843 - val_loss: 1.5223 - val_acc: 0.6913\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 175s 44ms/step - loss: 0.5855 - acc: 0.9905 - val_loss: 1.5488 - val_acc: 0.7200\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 0.5631 - acc: 0.9888 - val_loss: 1.6739 - val_acc: 0.7063\n"
     ]
    }
   ],
   "source": [
    "#  epoch=20, batch_size=32, l2=0.01\n",
    "model, images_test, labels_test = fit_cnn_model(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 108, 196, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 108, 196, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 54, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 338688)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                10838048  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 10,840,455\n",
      "Trainable params: 10,840,263\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = set_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 213s 53ms/step - loss: 2.6854 - acc: 0.5680 - val_loss: 2.2524 - val_acc: 0.6613\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 206s 52ms/step - loss: 2.0881 - acc: 0.7245 - val_loss: 2.0822 - val_acc: 0.6825\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 215s 54ms/step - loss: 1.8619 - acc: 0.7578 - val_loss: 1.9750 - val_acc: 0.6875\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 214s 53ms/step - loss: 1.6988 - acc: 0.7880 - val_loss: 1.9450 - val_acc: 0.6650\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 207s 52ms/step - loss: 1.5735 - acc: 0.8153 - val_loss: 1.8914 - val_acc: 0.6925\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 1.4885 - acc: 0.8287 - val_loss: 1.8571 - val_acc: 0.6825\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 220s 55ms/step - loss: 1.3647 - acc: 0.8580 - val_loss: 1.7507 - val_acc: 0.7100\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 214s 54ms/step - loss: 1.2388 - acc: 0.8855 - val_loss: 1.7454 - val_acc: 0.6863\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 216s 54ms/step - loss: 1.1302 - acc: 0.9140 - val_loss: 1.6884 - val_acc: 0.7188\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 207s 52ms/step - loss: 1.0396 - acc: 0.9333 - val_loss: 1.7737 - val_acc: 0.6837\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.9530 - acc: 0.9447 - val_loss: 1.9178 - val_acc: 0.6875\n",
      "Epoch 12/20\n",
      "3968/4000 [============================>.] - ETA: 1s - loss: 0.9154 - acc: 0.9498"
     ]
    }
   ],
   "source": [
    "# with dropout = 0.50, epoch=20, batch_size=32, l2=0.01\n",
    "model, images_test, labels_test = fit_cnn_model(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "test_model=load_model('/Users/basselhaidar/Desktop/Final Project/saved_models/CNN_Run_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "_, _, images_test, labels_test, _, _ = split_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_img_predict=test_model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(labels_test, cnn_img_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=None):\n",
    "    #=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "#     classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Benign keratosis-like lesions ', 'Melanocytic nevi',\n",
       "       'Dermatofibroma', 'Melanoma', 'Vascular lesions',\n",
       "       'Basal cell carcinoma', 'Actinic keratoses'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 108, 196, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 108, 196, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 338688)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                10838048  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 10,840,455\n",
      "Trainable params: 10,840,263\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 800 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 186s 46ms/step - loss: 2.3756 - acc: 0.6230 - val_loss: 1.8823 - val_acc: 0.6525\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 195s 49ms/step - loss: 1.6106 - acc: 0.6860 - val_loss: 1.5472 - val_acc: 0.6737\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 203s 51ms/step - loss: 1.2812 - acc: 0.7005 - val_loss: 1.6707 - val_acc: 0.6525\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 200s 50ms/step - loss: 1.0791 - acc: 0.7085 - val_loss: 1.3982 - val_acc: 0.6550\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 199s 50ms/step - loss: 0.9555 - acc: 0.7140 - val_loss: 1.0937 - val_acc: 0.6475\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 193s 48ms/step - loss: 0.8802 - acc: 0.7215 - val_loss: 1.1919 - val_acc: 0.6763\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 187s 47ms/step - loss: 0.8304 - acc: 0.7255 - val_loss: 1.1699 - val_acc: 0.6325\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 0.7850 - acc: 0.7343 - val_loss: 2.2540 - val_acc: 0.6525\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.7450 - acc: 0.7463 - val_loss: 2.5930 - val_acc: 0.3162\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 190s 48ms/step - loss: 0.7302 - acc: 0.7465 - val_loss: 1.5177 - val_acc: 0.6562\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.7183 - acc: 0.7510 - val_loss: 1.6203 - val_acc: 0.5200\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 195s 49ms/step - loss: 0.7075 - acc: 0.7515 - val_loss: 2.1600 - val_acc: 0.4437\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 195s 49ms/step - loss: 0.6766 - acc: 0.7615 - val_loss: 1.5082 - val_acc: 0.6375\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.6405 - acc: 0.7770 - val_loss: 0.8745 - val_acc: 0.6925\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 195s 49ms/step - loss: 0.6496 - acc: 0.7692 - val_loss: 1.5862 - val_acc: 0.5212\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 198s 49ms/step - loss: 0.6122 - acc: 0.7845 - val_loss: 1.2207 - val_acc: 0.6887\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 192s 48ms/step - loss: 0.5959 - acc: 0.7980 - val_loss: 2.1488 - val_acc: 0.6637\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 190s 48ms/step - loss: 0.5801 - acc: 0.8050 - val_loss: 1.8102 - val_acc: 0.6450\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.5420 - acc: 0.8210 - val_loss: 3.7632 - val_acc: 0.6562\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.4947 - acc: 0.8385 - val_loss: 2.8924 - val_acc: 0.6538\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 0.4609 - acc: 0.8560 - val_loss: 1.9106 - val_acc: 0.6425\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 193s 48ms/step - loss: 0.4017 - acc: 0.8798 - val_loss: 4.3087 - val_acc: 0.3013\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 184s 46ms/step - loss: 0.3423 - acc: 0.9042 - val_loss: 3.3710 - val_acc: 0.6112\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 184s 46ms/step - loss: 0.2846 - acc: 0.9190 - val_loss: 4.0888 - val_acc: 0.6562\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.4323 - acc: 0.8702 - val_loss: 5.0026 - val_acc: 0.6475\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 190s 47ms/step - loss: 0.3966 - acc: 0.8812 - val_loss: 4.5850 - val_acc: 0.6538\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.2764 - acc: 0.9220 - val_loss: 3.2573 - val_acc: 0.6525\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.2557 - acc: 0.9330 - val_loss: 1.7926 - val_acc: 0.5938\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 194s 48ms/step - loss: 0.2393 - acc: 0.9382 - val_loss: 1.5685 - val_acc: 0.6388\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 186s 47ms/step - loss: 0.1845 - acc: 0.9560 - val_loss: 1.3611 - val_acc: 0.6500\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 191s 48ms/step - loss: 0.1552 - acc: 0.9650 - val_loss: 1.5687 - val_acc: 0.6350\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.1050 - acc: 0.9822 - val_loss: 1.9447 - val_acc: 0.6613\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.1204 - acc: 0.9708 - val_loss: 1.9335 - val_acc: 0.6775\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.0828 - acc: 0.9870 - val_loss: 1.9675 - val_acc: 0.6737\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 190s 47ms/step - loss: 0.0578 - acc: 0.9928 - val_loss: 1.9266 - val_acc: 0.6813\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 186s 46ms/step - loss: 0.0444 - acc: 0.9965 - val_loss: 1.9668 - val_acc: 0.6813\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 185s 46ms/step - loss: 0.0461 - acc: 0.9942 - val_loss: 2.1165 - val_acc: 0.6325\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 0.0405 - acc: 0.9963 - val_loss: 1.8725 - val_acc: 0.6288\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 0.0322 - acc: 0.9970 - val_loss: 2.9710 - val_acc: 0.6425\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 189s 47ms/step - loss: 0.0551 - acc: 0.9890 - val_loss: 2.4094 - val_acc: 0.6512\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 188s 47ms/step - loss: 0.0661 - acc: 0.9868 - val_loss: 3.3412 - val_acc: 0.6162\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 194s 48ms/step - loss: 0.1077 - acc: 0.9702 - val_loss: 2.3863 - val_acc: 0.6188\n",
      "Epoch 43/100\n",
      "1888/4000 [=============>................] - ETA: 1:36 - loss: 0.0813 - acc: 0.9831"
     ]
    }
   ],
   "source": [
    "# optimize = SGD(lr=1e-2, momentum=0.9, decay=1e-2/epoch)\n",
    "# epoch=100, batch_num=32\n",
    "cnn = set_cnn_model()\n",
    "model, images_test, labels_test = fit_cnn_model(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 7 classes.\n",
      "Found 6415 images belonging to 7 classes.\n",
      "Found 1600 images belonging to 7 classes.\n",
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/flatiron/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 117s 29ms/step - loss: 90.5680 - acc: 0.3770 - val_loss: 84.8041 - val_acc: 0.4762\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 78.0681 - acc: 0.7658 - val_loss: 72.7782 - val_acc: 0.5688\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 66.7216 - acc: 0.8978 - val_loss: 63.1991 - val_acc: 0.6288\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 57.7882 - acc: 0.9380 - val_loss: 55.3970 - val_acc: 0.6250\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 50.7603 - acc: 0.9710 - val_loss: 49.3477 - val_acc: 0.6413\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 45.1545 - acc: 0.9845 - val_loss: 44.3518 - val_acc: 0.6400\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 40.5980 - acc: 0.9890 - val_loss: 40.3499 - val_acc: 0.6462\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 36.8257 - acc: 0.9920 - val_loss: 37.0066 - val_acc: 0.6500\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 33.6509 - acc: 0.9942 - val_loss: 34.2005 - val_acc: 0.6513\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 30.9408 - acc: 0.9970 - val_loss: 31.7001 - val_acc: 0.6525\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 28.6148 - acc: 0.9967 - val_loss: 29.5810 - val_acc: 0.6550\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 105s 26ms/step - loss: 26.5933 - acc: 0.9962 - val_loss: 27.7399 - val_acc: 0.6537\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 113s 28ms/step - loss: 24.8131 - acc: 0.9962 - val_loss: 26.0478 - val_acc: 0.6525\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 113s 28ms/step - loss: 23.2403 - acc: 0.9980 - val_loss: 24.6806 - val_acc: 0.6537\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 112s 28ms/step - loss: 21.8451 - acc: 0.9977 - val_loss: 23.4407 - val_acc: 0.6537\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 112s 28ms/step - loss: 20.5918 - acc: 0.9985 - val_loss: 22.4192 - val_acc: 0.6537\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 112s 28ms/step - loss: 19.4712 - acc: 0.9972 - val_loss: 21.3453 - val_acc: 0.6537\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 110s 28ms/step - loss: 18.4525 - acc: 0.9985 - val_loss: 20.2847 - val_acc: 0.6512\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 109s 27ms/step - loss: 17.5312 - acc: 0.9982 - val_loss: 19.4716 - val_acc: 0.6513\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 110s 27ms/step - loss: 16.6886 - acc: 0.9990 - val_loss: 18.5888 - val_acc: 0.6512\n"
     ]
    }
   ],
   "source": [
    "new_imagenet, images_test_imagenet, labels_test_imagenet= imagenet_classifier(epoch=20, batch=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
